{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq4Pkal6_lOc"
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GUi10Dvi_lOf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "billsum = load_dataset(\"xsum\", split=\"train[:2%]\").filter(lambda x: len(x[\"document\"]) <= 1000)\n",
        "billsumTest = load_dataset(\"xsum\", split=\"test[:1%]\").filter(lambda x: len(x[\"document\"]) <= 1000)\n",
        "billsumEval = load_dataset(\"xsum\", split=\"validation[:1%]\").filter(lambda x: len(x[\"document\"]) <= 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "929"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(billsum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GvQbvp60_lOh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling, GPT2LMHeadModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainingArguments, Trainer\n",
        "from transformers import GPT2Tokenizer, GPT2Config\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", max_length=1000, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77nA_KXI_lOg"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [ \"summarize: \" + doc1  + \"TL.DR \" + doc2 for doc1,doc2 in zip(examples[\"document\"],examples[\"summary\"])]\n",
        "    model_inputs = tokenizer(inputs,return_tensors='pt', truncation=True, max_length=1000, padding=True)\n",
        "\n",
        "    labels = tokenizer(text_target=inputs,return_tensors='pt', max_length=1000, truncation=True, padding=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "929"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(billsum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC0j7Md8_lOh"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nw5BM9lb_lOh"
      },
      "outputs": [],
      "source": [
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
        "tokenized_billsumTest = billsum.map(preprocess_function, batched=True)\n",
        "tokenized_billsumEval = billsum.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|â–Ž         | 53/1860 [06:04<3:18:45,  6.60s/it]"
          ]
        }
      ],
      "source": [
        "def train(output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          resume_from_checkpoint):\n",
        "\n",
        "  training_args = Seq2SeqTrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          logging_strategy=\"epoch\",\n",
        "          learning_rate=2e-5,\n",
        "          weight_decay=0.01,\n",
        "          save_total_limit=3,\n",
        "          predict_with_generate=True,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "          evaluation_strategy=\"epoch\",\n",
        "          per_device_eval_batch_size=per_device_train_batch_size,\n",
        "        #   fp16=True,\n",
        "      )\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          train_dataset=tokenized_billsum,\n",
        "          tokenizer=tokenizer,\n",
        "          data_collator=data_collator,\n",
        "          eval_dataset=tokenized_billsumEval,\n",
        "          compute_metrics=compute_metrics,\n",
        "  )\n",
        "      \n",
        "  trainer.train(resume_from_checkpoint = resume_from_checkpoint)\n",
        "  trainer.save_model()\n",
        "  return trainer\n",
        "\n",
        "# Train\n",
        "TrainerMain = train(\n",
        "    output_dir='outputdir2',\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=10,\n",
        "    num_train_epochs=20,\n",
        "    resume_from_checkpoint = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt8NpA8X_lOi"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(generate_text(billsum[33][\"document\"], max_new_tokens = 50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainerMain.state.log_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = TrainerMain.state.log_history\n",
        "# Rozdzielanie danych\n",
        "epoch = [entry['epoch'] for entry in data if 'loss' in entry]\n",
        "loss_values = [entry['loss'] for entry in data if 'loss' in entry]\n",
        "eval_loss_values = [entry['eval_loss'] for entry in data if 'eval_loss' in entry]\n",
        "\n",
        "# Tworzenie wykresÃ³w\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Wykres funkcji straty\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch, loss_values, marker='o', linestyle='-', color='b')\n",
        "plt.title('Funkcja Straty')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "# Wykres funkcji straty ewaluacyjnej\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch, eval_loss_values, marker='o', linestyle='-', color='r')\n",
        "\n",
        "plt.axhline(y=1, color='green', linestyle='-', label='Linia pozioma')\n",
        "\n",
        "plt.title('Funkcja wyniku ROUGE-L')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('ROUGE-L')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
