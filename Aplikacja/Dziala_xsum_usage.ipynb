{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq4Pkal6_lOc"
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUi10Dvi_lOf"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# billsum = load_dataset(\"xsum\", split=\"train[:5%]\").filter(lambda x: len(x[\"document\"]) <= 1000)\n",
        "# billsumTest = load_dataset(\"xsum\", split=\"test[:5%]\").filter(lambda x: len(x[\"document\"]) <= 1000)\n",
        "billsumEval = load_dataset(\"xsum\", split=\"validation[:5%]\").filter(lambda x: len(x[\"document\"]) <= 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GvQbvp60_lOh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling, GPT2LMHeadModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainingArguments, Trainer\n",
        "from transformers import GPT2Tokenizer, GPT2Config\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", max_length=1000)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"outputdir\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'GPT2LMHeadModel' object has no attribute 'loss'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'loss'"
          ]
        }
      ],
      "source": [
        "model.loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(sequence, max_new_tokens = 50, min_new_tokens = 10, temperature = 1, top_k = 50, top_p = 1.0, rep_penalty = 1.0, model_trained = True):\n",
        "    if model_trained:\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"outputdir\")\n",
        "    else:\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        \n",
        "    text = \"\\n\\nSummarize: \" + sequence + \"\\n\\n\"\n",
        "    input = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    output = model.generate(input[\"input_ids\"], attention_mask=input[\"attention_mask\"], max_new_tokens = max_new_tokens, do_sample=True, min_new_tokens=min_new_tokens, temperature = temperature, top_k = top_k, top_p = top_p, repetition_penalty = rep_penalty, pad_token_id=tokenizer.eos_token_id)[0]\n",
        "\n",
        "    return  tokenizer.decode(output)[len(text):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rouge.compute(predictions=[\"Hello\"],references=[\"Hello\"])['rouge1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WynikiGPT2U_temp = []\n",
        "WynikiGPT2_temp = []\n",
        "for temp in range(1, 21):\n",
        "    sumModel = 0   \n",
        "    sumModelGPT2 = 0\n",
        "    for i in range(1,6):\n",
        "        print(\"(TEMP: \" + str(temp/10) + \", ID: \" + str(i) + \")\")\n",
        "        sumModel = sumModel + rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], temperature = temp/10, top_k = 30)],references=[billsumEval[i][\"summary\"]])['rougeL']\n",
        "        sumModelGPT2 = sumModelGPT2 + (rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], temperature = temp/10, top_k = 30, model_trained = False)],references=[billsumEval[i][\"summary\"]]))['rougeL']\n",
        "    WynikiGPT2U_temp.append(sumModel/5)\n",
        "    WynikiGPT2_temp.append(sumModelGPT2/5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WynikiGPT2U_topP = []\n",
        "WynikiGPT2_topP = []\n",
        "for top_p in range(1, 11):\n",
        "    sumModel = 0   \n",
        "    sumModelGPT2 = 0\n",
        "    for i in range(1,6):\n",
        "        print(\"(topP: \" + str(top_p/10) + \", ID: \" + str(i) + \")\")\n",
        "        sumModel = sumModel + rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], top_p = top_p/10)],references=[billsumEval[i][\"summary\"]])['rougeL']\n",
        "        sumModelGPT2 = sumModelGPT2 + (rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], top_p = top_p/10, model_trained = False)],references=[billsumEval[i][\"summary\"]]))['rougeL']\n",
        "    WynikiGPT2U_topP.append(sumModel/5)\n",
        "    WynikiGPT2_topP.append(sumModelGPT2/5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WynikiGPT2U_topK = []\n",
        "WynikiGPT2_topK = []\n",
        "for topK in range(2, 50, 2):\n",
        "    sumModel = 0   \n",
        "    sumModelGPT2 = 0\n",
        "    for i in range(1,6):\n",
        "        print(\"(topK: \" + str(topK) + \", ID: \" + str(i) + \")\")\n",
        "        sumModel = sumModel + rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], top_k = topK)],references=[billsumEval[i][\"summary\"]])['rougeL']\n",
        "        sumModelGPT2 = sumModelGPT2 + (rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], top_k = topK, model_trained = False)],references=[billsumEval[i][\"summary\"]]))['rougeL']\n",
        "    WynikiGPT2U_topK.append(sumModel/5)\n",
        "    WynikiGPT2_topK.append(sumModelGPT2/5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WynikiGPT2U_repPenalty = []\n",
        "WynikiGPT2_repPenalty = []\n",
        "for repPenalty in range(1, 21):\n",
        "    sumModel = 0   \n",
        "    sumModelGPT2 = 0\n",
        "    for i in range(1,6):\n",
        "        print(\"(repPenalty: \" + str(repPenalty/10) + \", ID: \" + str(i) + \")\")\n",
        "        sumModel = sumModel + rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], rep_penalty = repPenalty/10)],references=[billsumEval[i][\"summary\"]])['rougeL']\n",
        "        sumModelGPT2 = sumModelGPT2 + (rouge.compute(predictions=[generate_text(billsumEval[i][\"document\"], rep_penalty = repPenalty/10, model_trained = False)],references=[billsumEval[i][\"summary\"]]))['rougeL']\n",
        "    WynikiGPT2U_repPenalty.append(sumModel/5)\n",
        "    WynikiGPT2_repPenalty.append(sumModelGPT2/5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# TEMP\n",
        "\n",
        "# Tworzenie wykresu\n",
        "plt.plot([x/10 for x in list(range(1, 21))], WynikiGPT2U_temp, label='Trained', color='green')\n",
        "plt.plot([x/10 for x in list(range(1, 21))], WynikiGPT2_temp, label='Basic')\n",
        "\n",
        "\n",
        "# Dodanie poziomej linii na wartości y=7\n",
        "plt.axhline(y=1, color='r', linestyle='-', label='Optymalny wynik')\n",
        "\n",
        "# Dodanie legendy\n",
        "plt.legend()\n",
        "plt.title('Funkcja wyniku ROUGE-L')\n",
        "plt.xlabel('temperature')\n",
        "plt.ylabel('ROUGE-L')\n",
        "\n",
        "# Wyświetlenie wykresu\n",
        "plt.show()\n",
        "\n",
        "#TOP_P\n",
        "\n",
        "# Tworzenie wykresu\n",
        "plt.plot([x/10 for x in list(range(1, 11))], WynikiGPT2U_topP, label='Trained', color='green')\n",
        "plt.plot([x/10 for x in list(range(1, 11))], WynikiGPT2_topP, label='Basic')\n",
        "\n",
        "\n",
        "# Dodanie poziomej linii na wartości y=7\n",
        "plt.axhline(y=1, color='r', linestyle='-', label='Optymalny wynik')\n",
        "\n",
        "# Dodanie legendy\n",
        "plt.legend()\n",
        "plt.title('Funkcja wyniku ROUGE-L')\n",
        "plt.xlabel('TOP_P')\n",
        "plt.ylabel('ROUGE-L')\n",
        "\n",
        "# Wyświetlenie wykresu\n",
        "plt.show()\n",
        "\n",
        "# TOP_k\n",
        "\n",
        "# Tworzenie wykresu\n",
        "plt.plot([x for x in list(range(2, 50, 2))], WynikiGPT2U_topK, label='Trained', color='green')\n",
        "plt.plot([x for x in list(range(2, 50, 2))], WynikiGPT2_topK, label='Basic')\n",
        "\n",
        "\n",
        "# Dodanie poziomej linii na wartości y=7\n",
        "plt.axhline(y=1, color='r', linestyle='-', label='Optymalny wynik')\n",
        "\n",
        "# Dodanie legendy\n",
        "plt.legend()\n",
        "plt.title('Funkcja wyniku ROUGE-L')\n",
        "plt.xlabel('TOP_k')\n",
        "plt.ylabel('ROUGE-L')\n",
        "\n",
        "# Wyświetlenie wykresu\n",
        "plt.show()\n",
        "\n",
        "# Repetition Penalty\n",
        "\n",
        "# Tworzenie wykresu\n",
        "plt.plot([x/10 for x in list(range(1, 21))], WynikiGPT2U_repPenalty, label='Trained', color='green')\n",
        "plt.plot([x/10 for x in list(range(1, 21))], WynikiGPT2_repPenalty, label='Basic')\n",
        "\n",
        "\n",
        "# Dodanie poziomej linii na wartości y=7\n",
        "plt.axhline(y=1, color='r', linestyle='-', label='Optymalny wynik')\n",
        "\n",
        "# Dodanie legendy\n",
        "plt.legend()\n",
        "plt.title('Funkcja wyniku ROUGE-L')\n",
        "plt.xlabel('Repetition Penalty')\n",
        "plt.ylabel('ROUGE-L')\n",
        "\n",
        "# Wyświetlenie wykresu\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
